

## 序列建模和注意力机制

- [Sequence to Sequence Learning with Neural Networks]()
    - 2014年，由 Sutskever, Vinyals 提出 seq2seq 模型
- [Neural Machine Translation by Jointly Learning to Align and Translate]()
    - 2015年，由 Bahdanau, Cho, Bengio 提出给予注意力机制的 NMT 模型
- [A Structured Self-Attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)
    - 2017年，自注意力机制，由 Yoshua Bengio 组提出
    - 对长度为 N 的句子生成一个 N * r 的注意力向量，表示 r 个语义下的注意力权重。和原句子加权后得到句子向量。
- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
    - 由 Vaswani 等人提出 Transformer 架构


## 预训练和大规模语言模型
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]()
    - 2018 年，由谷歌提出 BERT 预训练模型。
- [Improving Language Understanding by Generative Pre-Training]()
    - 2018年，Radford 提出 GPT 模型。
- [Language Models are Unsupervised Multitask Learners]()
    - 2019年，GPT-2 模型
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer]()
    - 2020年，T5模型
- [GPT-3: Language Models are Few-Shot Learners]()
    - 2020年，GPT-3 模型

## 多模态模型和超大规模模型
- [Learning Transferable Visual Models From Natural Language Supervision]()
    - CLIP 模型
- [PaLM: Scaling Language Modeling with Pathways]()
    - PaLM 模型
- [LLaMA: Open and Efficient Foundation Language Models]
    - LLaMA 模型
